## 网页链路分析

链路分析旨在通过前段埋点数据分析出用户访问链路的过程。

原始数据为json格式，

下面展示使用不同框架在计算过程中的代码量，请注意，其本质区别还是计算性能和实时要求的区别。

原始数据是json格式进行存放的，在试验阶段使用下述简化的数据。

```yaml
user_id,timestamp,page
1,2024-05-01T10:00:00,home
1,2024-05-01T10:01:00,about
1,2024-05-01T10:02:00,contact
1,2024-05-01T10:03:00,products
2,2024-05-01T10:00:00,home
2,2024-05-01T10:01:00,products
2,2024-05-01T10:02:00,product_details
3,2024-05-01T10:00:00,home
3,2024-05-01T10:01:00,about
3,2024-05-01T10:03:00,contact
4,2024-05-01T10:00:00,home
4,2024-05-01T10:02:00,about
4,2024-05-01T10:04:00,products
4,2024-05-01T10:06:00,product_details
5,2024-05-01T10:00:00,home
5,2024-05-01T10:01:00,contact
5,2024-05-01T10:02:00,products
5,2024-05-01T10:04:00,product_details
6,2024-05-01T10:00:00,home
6,2024-05-01T10:02:00,about
6,2024-05-01T10:04:00,contact
7,2024-05-01T10:00:00,home
7,2024-05-01T10:03:00,products
7,2024-05-01T10:04:00,product_details
8,2024-05-01T10:00:00,home
8,2024-05-01T10:02:00,about
8,2024-05-01T10:04:00,contact
8,2024-05-01T10:06:00,products
9,2024-05-01T10:00:00,home
9,2024-05-01T10:01:00,products
9,2024-05-01T10:03:00,product_details
10,2024-05-01T10:00:00,home
10,2024-05-01T10:01:00,about
10,2024-05-01T10:02:00,products
10,2024-05-01T10:03:00,product_details
```

### 计算用户访问链路的频率高低

|                      | Spark | Flink  |
| -------------------- | ----- | ------ |
| 计算用户路径时间     | 85ms  | 1689ms |
| 统计完全一致链路时间 | 10ms  | 42ms   |

Flink 计算

```java
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.GroupReduceFunction;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.util.Collector;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;

public class FrequentPathAnalysis {

    public static void main(String[] args) throws Exception {
        // 设置Flink执行环境
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

        // 读取用户访问数据
        DataSet<Tuple3<String, String, String>> visits = env.readCsvFile("path/to/visits.csv")
                .ignoreFirstLine()
                .fieldDelimiter(",")
                .types(String.class, String.class, String.class);

        // 对每个用户的访问数据进行排序并生成访问路径
        DataSet<Tuple2<String, String>> paths = visits
                .groupBy(0)
                .reduceGroup(new GroupReduceFunction<Tuple3<String, String, String>, Tuple2<String, String>>() {
                    @Override
                    public void reduce(Iterable<Tuple3<String, String, String>> values, Collector<Tuple2<String, String>> out) throws ParseException {
                        List<Tuple2<String, String>> visitsList = new ArrayList<>();
                        for (Tuple3<String, String, String> value : values) {
                            visitsList.add(new Tuple2<>(value.f1, value.f2));
                        }
                        visitsList.sort(Comparator.comparing(o -> new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss").parse(o.f0)));
                        StringBuilder path = new StringBuilder();
                        for (Tuple2<String, String> visit : visitsList) {
                            if (path.length() > 0) {
                                path.append(" -> ");
                            }
                            path.append(visit.f1);
                        }
                        out.collect(new Tuple2<>(visitsList.get(0).f0.split("T")[0], path.toString()));
                    }
                });

        // 统计每种访问路径的频率
        DataSet<Tuple2<String, Integer>> pathCounts = paths
                .map(path -> new Tuple2<>(path.f1, 1))
                .returns(Tuple2.class)
                .groupBy(0)
                .sum(1);

        // 找出最频繁的访问路径
        DataSet<Tuple2<String, Integer>> mostFrequentPath = pathCounts
                .reduceGroup(new GroupReduceFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
                    @Override
                    public void reduce(Iterable<Tuple2<String, Integer>> values, Collector<Tuple2<String, Integer>> out) {
                        String mostFrequentPath = null;
                        int maxCount = 0;
                        for (Tuple2<String, Integer> value : values) {
                            if (value.f1 > maxCount) {
                                maxCount = value.f1;
                                mostFrequentPath = value.f0;
                            }
                        }
                        out.collect(new Tuple2<>(mostFrequentPath, maxCount));
                    }
                });

        // 输出结果
        mostFrequentPath.print();
    }
}


```



Spark实现，Spark组件支持较多，很多函数可以开箱即用。

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

object FrequentPathAnalysis {
  def main(args: Array[String]): Unit = {
    // 创建SparkSession
    val spark = SparkSession.builder()
      .appName("Frequent Path Analysis")
      .getOrCreate()

    import spark.implicits._

    // 读取用户访问数据
    val visits = spark.read.option("header", "true")
      .csv("path/to/visits.csv")
      .withColumn("timestamp", to_timestamp($"timestamp"))

    // 窗口函数定义
    val windowSpec = Window.partitionBy("user_id").orderBy("timestamp")

    // 添加下一个页面列
    val visitsWithNextPage = visits
      .withColumn("next_page", lead("page", 1).over(windowSpec))
      .filter($"next_page".isNotNull)
      .withColumn("path", concat_ws(" -> ", $"page", $"next_page"))

    // 统计每种访问路径的频率
    val pathCounts = visitsWithNextPage.groupBy("path").count()

    // 找出最频繁的访问路径
    val mostFrequentPath = pathCounts.orderBy(desc("count")).limit(1)

    // 输出结果
    mostFrequentPath.show()

    // 停止SparkSession
    spark.stop()
  }
}

```



### 相似访问路径优化计算结果

Flink，聚类算法需要自己实现，可信度不如工具库高。

```java
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.GroupReduceFunction;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.functions.KeySelector;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.util.Collector;
import org.apache.flink.ml.common.distance.LevenshteinDistance;

import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;

public class PathClustering {

    public static void main(String[] args) throws Exception {
        // 创建Flink执行环境
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

        // 读取用户访问数据
        DataSet<Tuple3<String, String, String>> visits = env.readCsvFile("path/to/visits.csv")
                .ignoreFirstLine()
                .fieldDelimiter(",")
                .types(String.class, String.class, String.class);

        // 按用户和时间排序访问记录并生成路径
        DataSet<Tuple2<String, String>> userPaths = visits
                .groupBy(0)
                .reduceGroup(new GroupReduceFunction<Tuple3<String, String, String>, Tuple2<String, String>>() {
                    @Override
                    public void reduce(Iterable<Tuple3<String, String, String>> values, Collector<Tuple2<String, String>> out) throws Exception {
                        List<Tuple3<String, String, String>> sortedValues = new ArrayList<>();
                        for (Tuple3<String, String, String> value : values) {
                            sortedValues.add(value);
                        }
                        sortedValues.sort(Comparator.comparing(o -> new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss").parse(o.f1)));
                        StringBuilder path = new StringBuilder();
                        for (Tuple3<String, String, String> visit : sortedValues) {
                            if (path.length() > 0) {
                                path.append(" -> ");
                            }
                            path.append(visit.f2);
                        }
                        out.collect(new Tuple2<>(sortedValues.get(0).f0, path.toString()));
                    }
                });

        // 对路径进行相似性计算和聚类
        DataSet<Tuple2<String, String>> clusteredPaths = userPaths
                .groupBy(new KeySelector<Tuple2<String, String>, String>() {
                    @Override
                    public String getKey(Tuple2<String, String> value) {
                        return value.f1;
                    }
                })
                .reduceGroup(new GroupReduceFunction<Tuple2<String, String>, Tuple2<String, String>>() {
                    @Override
                    public void reduce(Iterable<Tuple2<String, String>> values, Collector<Tuple2<String, String>> out) {
                        List<Tuple2<String, String>> paths = new ArrayList<>();
                        for (Tuple2<String, String> value : values) {
                            paths.add(value);
                        }

                        // 简单的相似性计算和聚类逻辑
                        while (!paths.isEmpty()) {
                            Tuple2<String, String> reference = paths.remove(0);
                            List<Tuple2<String, String>> cluster = new ArrayList<>();
                            cluster.add(reference);
                            paths.removeIf(path -> {
                                int distance = LevenshteinDistance.compute(reference.f1, path.f1);
                                if (distance < 3) { // 这里可以调整相似度阈值
                                    cluster.add(path);
                                    return true;
                                }
                                return false;
                            });
                            for (Tuple2<String, String> path : cluster) {
                                out.collect(new Tuple2<>(path.f0, reference.f1));
                            }
                        }
                    }
                });

        // 输出结果
        clusteredPaths.print();
    }
}

```

Spark

```scala
import org.apache.spark.sql.{SparkSession, functions => F}
import org.apache.spark.sql.expressions.Window
import org.apache.spark.ml.feature.{Tokenizer, HashingTF, IDF}
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.ml.linalg.Vector

object PathClustering {
  def main(args: Array[String]): Unit = {
    // 创建SparkSession
    val spark = SparkSession.builder()
      .appName("Path Clustering")
      .getOrCreate()

    import spark.implicits._

    // 读取用户访问数据
    val visits = spark.read.option("header", "true")
      .csv("path/to/visits.csv")
      .withColumn("timestamp", F.to_timestamp($"timestamp"))

    // 窗口函数定义
    val windowSpec = Window.partitionBy("user_id").orderBy("timestamp")

    // 添加下一个页面列
    val visitsWithNextPage = visits
      .withColumn("next_page", F.lead("page", 1).over(windowSpec))
      .filter($"next_page".isNotNull)
      .withColumn("path", F.concat_ws(" -> ", $"page", $"next_page"))

    // 合并路径为序列
    val userPaths = visitsWithNextPage
      .groupBy("user_id")
      .agg(F.collect_list("path").as("paths"))

    // 转换为字符串序列
    val pathsString = userPaths
      .withColumn("sequence", F.concat_ws(" ", $"paths"))

    // 使用TF-IDF表示路径序列
    val tokenizer = new Tokenizer().setInputCol("sequence").setOutputCol("words")
    val wordsData = tokenizer.transform(pathsString)

    val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(20)
    val featurizedData = hashingTF.transform(wordsData)

    val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
    val idfModel = idf.fit(featurizedData)
    val rescaledData = idfModel.transform(featurizedData)

    // 聚类
    val kmeans = new KMeans().setK(5).setSeed(1L)
    val model = kmeans.fit(rescaledData)

    val predictions = model.transform(rescaledData)

    // 输出结果
    predictions.select("user_id", "paths", "prediction").show()

    // 停止SparkSession
    spark.stop()
  }
}

```

Mapreduce实现

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.*;

public class PathClustering {

    public static class PathMapper extends Mapper<Object, Text, Text, Text> {
        private Text userId = new Text();
        private Text path = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] fields = value.toString().split(",");
            if (fields.length == 3) {
                userId.set(fields[0]);
                path.set(fields[1] + "," + fields[2]);
                context.write(userId, path);
            }
        }
    }

    public static class PathReducer extends Reducer<Text, Text, Text, Text> {
        private Text result = new Text();

        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            List<String[]> records = new ArrayList<>();
            for (Text val : values) {
                String[] parts = val.toString().split(",");
                records.add(parts);
            }
            records.sort(Comparator.comparing(o -> {
                try {
                    return new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss").parse(o[0]);
                } catch (ParseException e) {
                    throw new RuntimeException(e);
                }
            }));
            StringBuilder path = new StringBuilder();
            for (String[] record : records) {
                if (path.length() > 0) {
                    path.append(" -> ");
                }
                path.append(record[1]);
            }
            result.set(path.toString());
            context.write(key, result);
        }
    }

    public static class ClusterMapper extends Mapper<Object, Text, Text, IntWritable> {
        private Text path = new Text();
        private final static IntWritable one = new IntWritable(1);

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] fields = value.toString().split("\t");
            if (fields.length == 2) {
                path.set(fields[1]);
                context.write(path, one);
            }
        }
    }

    public static class ClusterReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static class MergeReducer extends Reducer<Text, IntWritable, Text, Text> {
        private Text cluster = new Text();

        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            List<String> paths = new ArrayList<>();
            for (IntWritable val : values) {
                paths.add(key.toString());
            }
            String mergedPath = mergeSimilarPaths(paths);
            cluster.set(mergedPath);
            context.write(key, cluster);
        }

        private String mergeSimilarPaths(List<String> paths) {
            // 简单的相似性计算和聚类逻辑，可以根据需要进行调整
            while (!paths.isEmpty()) {
                String reference = paths.remove(0);
                List<String> cluster = new ArrayList<>();
                cluster.add(reference);
                paths.removeIf(path -> {
                    int distance = computeLevenshteinDistance(reference, path);
                    return distance < 3; // 这里可以调整相似度阈值
                });
                return String.join(", ", cluster);
            }
            return "";
        }

        private int computeLevenshteinDistance(String s1, String s2) {
            int[] costs = new int[s2.length() + 1];
            for (int i = 0; i <= s1.length(); i++) {
                int lastValue = i;
                for (int j = 0; j <= s2.length(); j++) {
                    if (i == 0)
                        costs[j] = j;
                    else {
                        if (j > 0) {
                            int newValue = costs[j - 1];
                            if (s1.charAt(i - 1) != s2.charAt(j - 1))
                                newValue = Math.min(Math.min(newValue, lastValue), costs[j]) + 1;
                            costs[j - 1] = lastValue;
                            lastValue = newValue;
                        }
                    }
                }
                if (i > 0)
                    costs[s2.length()] = lastValue;
            }
            return costs[s2.length()];
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        if (otherArgs.length != 3) {
            System.err.println("Usage: PathClustering <in> <out1> <out2>");
            System.exit(2);
        }

        Job job1 = Job.getInstance(conf, "Path Generation");
        job1.setJarByClass(PathClustering.class);
        job1.setMapperClass(PathMapper.class);
        job1.setReducerClass(PathReducer.class);
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(Text.class);
        FileInputFormat.addInputPath(job1, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job1, new Path(otherArgs[1]));
        job1.waitForCompletion(true);

        Job job2 = Job.getInstance(conf, "Path Clustering");
        job2.setJarByClass(PathClustering.class);
        job2.setMapperClass(ClusterMapper.class);
        job2.setReducerClass(ClusterReducer.class);
        job2.setOutputKeyClass(Text.class);
        job2.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job2, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job2, new Path(otherArgs[2]));
        job2.waitForCompletion(true);

        Job job3 = Job.getInstance(conf, "Merge Clusters");
        job3.setJarByClass(PathClustering.class);
        job3.setMapperClass(ClusterMapper.class);
        job3.setReducerClass(MergeReducer.class);
        job3.setOutputKeyClass(Text.class);
        job3.setOutputValueClass(Text.class);
        FileInputFormat.addInputPath(job3, new Path(otherArgs[2]));
        FileOutputFormat.setOutputPath(job3, new Path(otherArgs[2] + "_merged"));
        System.exit(job3.waitForCompletion(true) ? 0 : 1);
    }
}

```

